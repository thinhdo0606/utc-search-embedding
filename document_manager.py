
import os
import json
import numpy as np
import faiss
import pickle
from datetime import datetime
from models import db, User, Document
import re

class DocumentManager:
    def __init__(self, model, data_dir='data', index_file='search_index.pkl'):
        self.model = model
        self.data_dir = data_dir
        self.index_file = index_file
        self.documents = []
        self.embeddings = None
        self.faiss_index = None
        self.metadata = []
        
        # T·∫°o th∆∞ m·ª•c data n·∫øu ch∆∞a c√≥
        os.makedirs(data_dir, exist_ok=True)
        
        # Load d·ªØ li·ªáu hi·ªán c√≥
        self.load_existing_data()
    
    def load_existing_data(self):
        """Load d·ªØ li·ªáu t·ª´ database"""
        print("ƒêang load d·ªØ li·ªáu t·ª´ database...")
        
        # X√≥a d·ªØ li·ªáu c≈©
        self.documents = []
        self.metadata = []
        self.embeddings = None
        self.faiss_index = None
        
        try:
            # Load t·ª´ database
            db_documents = Document.query.filter_by(is_active=True).order_by(Document.id).all()
            
            if db_documents:
                print(f"üîÑ ƒêang load {len(db_documents)} documents t·ª´ database...")
                for doc in db_documents:
                    self.documents.append(doc.content)
                    self.metadata.append({
                        'id': f'db_{doc.id}',
                        'db_id': doc.id,
                        'source': doc.source_type,
                        'source_file': doc.source_file,
                        'added_date': doc.added_date.isoformat() if doc.added_date else None,
                        'added_by': doc.added_by,
                        'type': 'database',
                        'line_number': doc.line_number,
                        'page_number': doc.page_number
                    })
                print(f"‚úÖ ƒê√£ load {len(self.documents)} documents t·ª´ database")
            else:
                print("Database ch∆∞a c√≥ documents n√†o")
                
        except Exception as e:
            print(f"‚ùå L·ªói khi load t·ª´ database: {e}")
        
        # Rebuild index t·ª´ d·ªØ li·ªáu ƒë√£ load
        if self.documents:
            print("üîß Building index t·ª´ documents...")
            self.rebuild_index()
        else:
            print("Kh√¥ng c√≥ documents ƒë·ªÉ build index")
        
        print(f"DocumentManager ƒë√£ s·∫µn s√†ng v·ªõi {len(self.documents)} documents")
    

    


    
    def add_text_document(self, text, metadata=None, user_id=None):
        """Th√™m document text m·ªõi (l∆∞u v√†o database)"""
        if not text or not text.strip():
            return False, "N·ªôi dung r·ªóng"
        
        try:
            # T·∫°o source_info t·ª´ metadata
            source_info = metadata if metadata else {}
            
            # T·∫°o document trong database
            doc = Document.create_from_text(
                content=text.strip(),
                source_type='manual',
                source_file=None,
                source_info=source_info,
                added_by=user_id
            )
            
            # L∆∞u v√†o database
            db.session.add(doc)
            db.session.commit()
            
            # Th√™m v√†o memory ƒë·ªÉ c√≥ th·ªÉ search ngay
            self.documents.append(text.strip())
            doc_metadata = {
                'id': f'db_{doc.id}',
                'db_id': doc.id,
                'source': 'manual',
                'added_date': doc.added_date.isoformat(),
                'added_by': user_id,
                'type': 'database',
                'length': len(text.strip())
            }
            self.metadata.append(doc_metadata)
            
            # C·∫≠p nh·∫≠t index
            success = self._add_to_index(text.strip())
            
            if success:
                return True, f"ƒê√£ th√™m document ID: {doc.id}"
            else:
                # Rollback n·∫øu l·ªói index
                self.documents.pop()
                self.metadata.pop()
                return False, "L·ªói khi c·∫≠p nh·∫≠t search index"
                
        except Exception as e:
            db.session.rollback()
            return False, f"L·ªói l∆∞u database: {str(e)}"
    
    def add_documents_from_pdf_file(self, file_path, user_id=None):
        """Th√™m documents t·ª´ file PDF (l∆∞u v√†o database)"""
        if not os.path.exists(file_path):
            return False, "File kh√¥ng t·ªìn t·∫°i"
        
        try:
            if file_path.endswith('.pdf'):
                documents = self._extract_from_pdf(file_path)
            else:
                return False, "Ch·ªâ h·ªó tr·ª£ file .pdf"
            
            if not documents:
                return False, "Kh√¥ng t√¨m th·∫•y n·ªôi dung trong file PDF"
            
            # Th√™m t·ª´ng document v√†o database
            added_count = 0
            source_file = os.path.basename(file_path)
            
            for i, doc_text in enumerate(documents):
                if len(doc_text.strip()) > 50:  # Ch·ªâ th√™m document ƒë·ªß d√†i
                    try:
                        # T·∫°o document trong database
                        doc = Document.create_from_text(
                            content=doc_text.strip(),
                            source_type='pdf',
                            source_file=source_file,
                            source_info={'pdf_extract_index': i},
                            added_by=user_id,
                            page_number=i + 1  # Gi·∫£ ƒë·ªãnh m·ªói document t·ª´ 1 page
                        )
                        
                        db.session.add(doc)
                        db.session.commit()
                        
                        # Th√™m v√†o memory
                        self.documents.append(doc_text.strip())
                        self.metadata.append({
                            'id': f'db_{doc.id}',
                            'db_id': doc.id,
                            'source': 'pdf',
                            'source_file': source_file,
                            'added_date': doc.added_date.isoformat(),
                            'added_by': user_id,
                            'type': 'database',
                            'page_number': i + 1
                        })
                        
                        # C·∫≠p nh·∫≠t index
                        self._add_to_index(doc_text.strip())
                        added_count += 1
                        
                    except Exception as doc_error:
                        print(f"L·ªói th√™m document {i}: {doc_error}")
                        db.session.rollback()
                        continue
            
            return True, f"ƒê√£ th√™m {added_count}/{len(documents)} documents t·ª´ file PDF"
            
        except Exception as e:
            return False, f"L·ªói x·ª≠ l√Ω file PDF: {str(e)}"
    

    
    def _extract_from_txt(self, file_path):
        """Tr√≠ch xu·∫•t t·ª´ file txt"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Chia th√†nh c√°c paragraph
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        # N·∫øu kh√¥ng c√≥ paragraph, chia theo d√≤ng
        if len(paragraphs) <= 1:
            paragraphs = [line.strip() for line in content.split('\n') if line.strip()]
        
        return paragraphs
    
    def _extract_from_pdf(self, file_path):
        """Tr√≠ch xu·∫•t t·ª´ file PDF (s·ª≠ d·ª•ng PyPDF2 nh∆∞ trong app.py)"""
        try:
            import PyPDF2
            
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text_content = []
                
                for page in pdf_reader.pages:
                    page_text = page.extract_text()
                    if page_text and page_text.strip():
                        # Chia th√†nh c√°c paragraph
                        paragraphs = page_text.split('\n\n')
                        for para in paragraphs:
                            para = para.strip()
                            if len(para) > 50:  # Ch·ªâ l·∫•y paragraph ƒë·ªß d√†i
                                text_content.append(para)
                
                return text_content
        except Exception as e:
            print(f"L·ªói ƒë·ªçc file PDF: {e}")
            return []


    
    def _add_to_index(self, text):
        """Th√™m m·ªôt document v√†o index hi·ªán c√≥"""
        try:
            # T·∫°o embedding cho text m·ªõi
            new_embedding = self.model.encode([text], convert_to_tensor=True)
            new_embedding_np = new_embedding.cpu().numpy()
            
            # Chu·∫©n h√≥a vector v·ªÅ ƒë·ªô d√†i 1 (ƒë·ªÉ s·ª≠ d·ª•ng cosine similarity)
            norms = np.linalg.norm(new_embedding_np, axis=1, keepdims=True) + 1e-12
            new_embedding_np = new_embedding_np / norms
            
            if self.faiss_index is None:
                # T·∫°o index m·ªõi n·∫øu ch∆∞a c√≥ - s·ª≠ d·ª•ng Inner Product cho cosine similarity
                embedding_dim = new_embedding_np.shape[1]
                self.faiss_index = faiss.IndexFlatIP(embedding_dim)
                self.embeddings = new_embedding_np
            else:
                # Th√™m v√†o index hi·ªán c√≥
                self.embeddings = np.vstack([self.embeddings, new_embedding_np])
            
            # Th√™m v√†o FAISS index
            self.faiss_index.add(new_embedding_np)
            
            return True
        except Exception as e:
            print(f"L·ªói th√™m v√†o index: {e}")
            return False
    
    def rebuild_index(self):
        """Rebuild to√†n b·ªô index t·ª´ documents hi·ªán c√≥"""
        if not self.documents:
            print("‚ùå Kh√¥ng c√≥ documents ƒë·ªÉ rebuild")
            return
        
        print(f"üîß Rebuilding index cho {len(self.documents)} documents...")
        
        try:
            # Reset tr∆∞·ªõc khi rebuild
            self.embeddings = None
            self.faiss_index = None
            
            # T·∫°o embeddings cho t·∫•t c·∫£ documents
            print("üîÑ T·∫°o embeddings...")
            embeddings = self.model.encode(self.documents, convert_to_tensor=True)
            embeddings_np = embeddings.cpu().numpy()
            
            # Chu·∫©n h√≥a ƒë·ªÉ s·ª≠ d·ª•ng cosine similarity
            norms = np.linalg.norm(embeddings_np, axis=1, keepdims=True) + 1e-12
            self.embeddings = embeddings_np / norms
            print(f"‚úÖ Embeddings shape: {self.embeddings.shape}")
            
            # T·∫°o FAISS index m·ªõi - s·ª≠ d·ª•ng Inner Product cho cosine similarity
            print("üîÑ T·∫°o FAISS index (cosine similarity)...")
            embedding_dim = self.embeddings.shape[1]
            self.faiss_index = faiss.IndexFlatIP(embedding_dim)
            self.faiss_index.add(self.embeddings)
            print(f"‚úÖ FAISS index v·ªõi {self.faiss_index.ntotal} vectors")
            
            print(f"‚úÖ Rebuild ho√†n th√†nh: {len(self.documents)} documents")
            
            # L∆∞u index
            self.save_index()
            
        except Exception as e:
            print(f"‚ùå L·ªói rebuild index: {e}")
            # Reset v·ªÅ tr·∫°ng th√°i an to√†n
            self.embeddings = None
            self.faiss_index = None
    

    
    def search(self, query, k=5, similarity_threshold=0.5):
        """T√¨m ki·∫øm documents v·ªõi c·∫£i ti·∫øn ƒë·ªô ch√≠nh x√°c"""
        if self.faiss_index is None or len(self.documents) == 0:
            return []
        
        # Ki·ªÉm tra ƒë·ªìng b·ªô gi·ªØa documents v√† embeddings
        if hasattr(self, 'embeddings') and self.embeddings is not None and len(self.documents) != self.embeddings.shape[0]:
            print(f"Warning: Documents ({len(self.documents)}) v√† embeddings ({self.embeddings.shape[0]}) kh√¥ng ƒë·ªìng b·ªô. Rebuilding index...")
            self.rebuild_index()
        
        try:
            # T·∫°o embedding cho query g·ªëc (kh√¥ng expand qu√° r·ªông)
            original_embedding = self.model.encode([query], convert_to_tensor=True)
            original_np = original_embedding.cpu().numpy()
            
            # Chu·∫©n h√≥a query embedding
            norms = np.linalg.norm(original_np, axis=1, keepdims=True) + 1e-12
            query_np = original_np / norms
            
            # T√¨m ki·∫øm v·ªõi FAISS (l·∫•y nhi·ªÅu h∆°n ƒë·ªÉ c√≥ th·ªÉ filter v√† re-rank)
            search_k = min(k * 5, len(self.documents))
            distances, indices = self.faiss_index.search(query_np, search_k)
        except Exception as e:
            print(f"‚ùå L·ªói trong qu√° tr√¨nh search: {e}")
            return []
        
        candidates = []
        for i, idx in enumerate(indices[0]):
            if idx >= len(self.documents):
                continue
            
            # V·ªõi normalized embeddings v√† IndexFlatIP, distances ch√≠nh l√† cosine similarity
            similarity = float(distances[0][i])
            
            # √Åp d·ª•ng ng∆∞·ª°ng similarity cao h∆°n
            if similarity >= similarity_threshold:
                doc_content = self.documents[idx]
                
                # T√≠nh lexical overlap score (quan tr·ªçng h∆°n)
                overlap_score = self._calculate_keyword_overlap(query, doc_content)
                
                # Ki·ªÉm tra relevance contextual
                context_score = self._calculate_context_relevance(query, doc_content)
                
                # Penalty cho nh·ªØng document c√≥ nhi·ªÅu t·ª´ kh√≥a kh√¥ng li√™n quan
                noise_penalty = self._calculate_noise_penalty(query, doc_content)
                
                # K·∫øt h·ª£p ƒëi·ªÉm s·ªë v·ªõi tr·ªçng s·ªë m·ªõi:
                # - 50% semantic similarity
                # - 30% keyword overlap  
                # - 20% context relevance
                # - Tr·ª´ noise penalty
                combined_score = (0.5 * similarity + 
                                0.3 * overlap_score + 
                                0.2 * context_score - 
                                0.1 * noise_penalty)
                
                # Ch·ªâ l·∫•y nh·ªØng k·∫øt qu·∫£ c√≥ ƒëi·ªÉm t·ªïng h·ª£p cao
                if combined_score >= 0.4:  # Ng∆∞·ª°ng t·ªïng h·ª£p cao h∆°n
                    candidates.append({
                        'index': int(idx),
                        'content': doc_content,
                        'metadata': self.metadata[idx] if idx < len(self.metadata) else {},
                        'score': float(combined_score),
                        'similarity': similarity,
                        'overlap_score': overlap_score,
                        'context_score': context_score,
                        'noise_penalty': noise_penalty,
                        'rank': 0,  # Will be set after sorting
                        'source': 'default'
                    })
        
        # Re-rank theo combined score
        candidates.sort(key=lambda x: x['score'], reverse=True)
        
        # L·ªçc th√™m b·∫±ng c√°ch ki·ªÉm tra s·ª± li√™n quan th·ª±c s·ª±
        filtered_candidates = []
        for candidate in candidates:
            if self._is_truly_relevant(query, candidate['content']):
                filtered_candidates.append(candidate)
        
        # Ch·ªâ l·∫•y top k v√† g√°n rank
        results = []
        for rank, candidate in enumerate(filtered_candidates[:k], 1):
            candidate['rank'] = rank
            results.append(candidate)
        
        return results
    
    def _expand_query_vietnamese(self, query):
        """M·ªü r·ªông query v·ªõi c√°c t·ª´ ƒë·ªìng nghƒ©a ti·∫øng Vi·ªát"""
        query_lower = query.lower()
        expansions = [query]  # Lu√¥n bao g·ªìm query g·ªëc
        
        # T·ª´ ƒëi·ªÉn ƒë·ªìng nghƒ©a cho c√°c thu·∫≠t ng·ªØ ph·ªï bi·∫øn
        synonyms_map = {
            'r√®n luy·ªán': ['ƒëi·ªÉm r√®n luy·ªán', 'ƒë√°nh gi√° r√®n luy·ªán', 'DRL', 'ƒëi·ªÉm DRL', 'r√®n luy·ªán sinh vi√™n', 'ƒê√ÅNH GI√Å R√àN LUY·ªÜN SINH VI√äN'],
            'c·ªë v·∫•n h·ªçc t·∫≠p': ['c·ªë v·∫•n', 't∆∞ v·∫•n h·ªçc t·∫≠p', 'ƒë√°nh gi√° c·ªë v·∫•n', 'c·ªë v·∫•n hoc tap', 'ƒê√ÅNH GI√Å C·ªê V·∫§N H·ªåC T·∫¨P'],
            'h·ªçc ph√≠': ['m·ª©c h·ªçc ph√≠', 'thu h·ªçc ph√≠', 'mi·ªÖn gi·∫£m h·ªçc ph√≠', 'hoc phi'],
            'h·ªçc b·ªïng': ['hoc bong', 'h·ªçc b·ªïng khuy·∫øn kh√≠ch', 'h·ªçc b·ªïng khuy·∫øn h·ªçc'],
            'sinh vi√™n': ['sinh vien', 'h·ªçc sinh', 'hoc sinh'],
            'gi·∫£ng vi√™n': ['giang vien', 'th·∫ßy c√¥', 'gi√°o vi√™n'],
            'tuy·ªÉn sinh': ['tuyen sinh', 'x√©t tuy·ªÉn', 'thi tuy·ªÉn'],
            'ƒë√†o t·∫°o': ['dao tao', 'ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o', 'chuong trinh dao tao'],
            'th∆∞ vi·ªán': ['thu vien', 'library', 'kho s√°ch'],
            'k√Ω t√∫c x√°': ['ki tuc xa', 'ktx', 'dormitory'],
            'ho·∫°t ƒë·ªông': ['hoat dong', 'sinh ho·∫°t', 't·ªï ch·ª©c']
        }
        
        # T√¨m c√°c t·ª´ kh√≥a ph√π h·ª£p v√† th√™m ƒë·ªìng nghƒ©a
        for key, synonyms in synonyms_map.items():
            if key in query_lower or any(syn in query_lower for syn in synonyms):
                for synonym in synonyms:
                    if synonym not in expansions:
                        expansions.append(synonym)
        
        return expansions[:5]  # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng ƒë·ªÉ tr√°nh qu√° t·∫£i
    
    def _calculate_keyword_overlap(self, query, document):
        """T√≠nh ƒëi·ªÉm overlap t·ª´ kh√≥a gi·ªØa query v√† document"""
        # Chu·∫©n h√≥a text
        query_clean = re.sub(r'[^\w\s]', ' ', query.lower())
        doc_clean = re.sub(r'[^\w\s]', ' ', document.lower())
        
        # T√°ch t·ª´ v√† lo·∫°i b·ªè t·ª´ ng·∫Øn
        query_words = set([w for w in query_clean.split() if len(w) > 2])
        doc_words = set([w for w in doc_clean.split() if len(w) > 2])
        
        if not query_words:
            return 0.0
        
        # T√≠nh t·ª∑ l·ªá t·ª´ tr√πng kh·ªõp
        intersection = len(query_words & doc_words)
        return intersection / len(query_words)
    
    def _calculate_context_relevance(self, query, document):
        """T√≠nh ƒëi·ªÉm li√™n quan theo ng·ªØ c·∫£nh"""
        query_lower = query.lower()
        doc_lower = document.lower()
        
        # ƒê·ªãnh nghƒ©a c√°c ch·ªß ƒë·ªÅ ch√≠nh v√† t·ª´ kh√≥a li√™n quan
        topic_keywords = {
            'h·ªçc_t·∫≠p': ['h·ªçc t·∫≠p', 'h·ªçc ph√≠', 'm√¥n h·ªçc', 't√≠n ch·ªâ', 'ƒëi·ªÉm', 'thi', 'ki·ªÉm tra', 'b√†i t·∫≠p', 'gi·∫£ng d·∫°y', 'ch∆∞∆°ng tr√¨nh'],
            'sinh_vi√™n': ['sinh vi√™n', 'h·ªçc sinh', 't√¢n sinh vi√™n', 'c·ª±u sinh vi√™n', 'l·ªõp', 'kh√≥a h·ªçc'],
            'gi·∫£ng_vi√™n': ['gi·∫£ng vi√™n', 'gi√°o vi√™n', 'th·∫ßy', 'c√¥', 'gi√°o s∆∞', 'ph√≥ gi√°o s∆∞', 'ti·∫øn sƒ©'],
            'h√†nh_ch√≠nh': ['ƒëƒÉng k√Ω', 'th·ªß t·ª•c', 'gi·∫•y t·ªù', 'ch·ª©ng nh·∫≠n', 'x√°c nh·∫≠n', 'ph√≤ng ban', 'vƒÉn ph√≤ng'],
            'c∆°_s·ªü_v·∫≠t_ch·∫•t': ['th∆∞ vi·ªán', 'ph√≤ng h·ªçc', 'gi·∫£ng ƒë∆∞·ªùng', 'ph√≤ng th√≠ nghi·ªám', 'k√Ω t√∫c x√°', 'ktx'],
            'ho·∫°t_ƒë·ªông': ['ho·∫°t ƒë·ªông', 's·ª± ki·ªán', 'h·ªôi th·∫£o', 'seminar', 'nghi√™n c·ª©u', 'khoa h·ªçc'],
            'quy_ƒë·ªãnh': ['quy ƒë·ªãnh', 'quy ch·∫ø', 'lu·∫≠t', 'ƒëi·ªÅu', 'kho·∫£n', 'ngh·ªã ƒë·ªãnh', 'th√¥ng t∆∞']
        }
        
        # X√°c ƒë·ªãnh ch·ªß ƒë·ªÅ c·ªßa query
        query_topics = []
        for topic, keywords in topic_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                query_topics.append(topic)
        
        if not query_topics:
            return 0.5  # ƒêi·ªÉm trung b√¨nh n·∫øu kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c ch·ªß ƒë·ªÅ
        
        # T√≠nh ƒëi·ªÉm li√™n quan theo ch·ªß ƒë·ªÅ
        relevance_score = 0.0
        for topic in query_topics:
            topic_keywords_in_doc = sum(1 for keyword in topic_keywords[topic] if keyword in doc_lower)
            if topic_keywords_in_doc > 0:
                relevance_score += topic_keywords_in_doc / len(topic_keywords[topic])
        
        return min(relevance_score / len(query_topics), 1.0)
    
    def _calculate_noise_penalty(self, query, document):
        """T√≠nh penalty cho document c√≥ nhi·ªÅu t·ª´ kh√≥a kh√¥ng li√™n quan"""
        doc_lower = document.lower()
        query_lower = query.lower()
        
        # Danh s√°ch t·ª´ kh√≥a "nhi·ªÖu" th∆∞·ªùng g√¢y nh·∫ßm l·∫´n
        noise_keywords = {
            'th·ªÉ_thao': ['ƒë√° c·∫ßu', 'b√≥ng b√†n', 'b√≥ng ƒë√°', 'b√≥ng chuy·ªÅn', 'c·∫ßu l√¥ng', 'tennis', 'b√≥ng r·ªï', 'v√µ thu·∫≠t', 'th·ªÉ d·ª•c', 'th·ªÉ thao'],
            'c√¢u_l·∫°c_b·ªô': ['clb', 'c√¢u l·∫°c b·ªô', 'club', 'nh√≥m', 'ƒë·ªôi'],
            'ƒë·ªãa_ƒëi·ªÉm_xa': ['h√† n·ªôi', 'h·ªì ch√≠ minh', 'ƒë√† n·∫µng', 'c·∫ßn th∆°'] if not any(city in query_lower for city in ['h√† n·ªôi', 'h·ªì ch√≠ minh', 'ƒë√† n·∫µng', 'c·∫ßn th∆°']) else [],
            'th√¥ng_tin_c√°_nh√¢n': ['s·ªë ƒëi·ªán tho·∫°i', 'email c√° nh√¢n', 'ƒë·ªãa ch·ªâ nh√†'] if not any(info in query_lower for info in ['li√™n h·ªá', 'th√¥ng tin']) else []
        }
        
        penalty = 0.0
        total_noise_words = 0
        
        for category, keywords in noise_keywords.items():
            noise_count = sum(1 for keyword in keywords if keyword in doc_lower)
            total_noise_words += len(keywords)
            
            # N·∫øu query kh√¥ng li√™n quan ƒë·∫øn category n√†y nh∆∞ng document c√≥ nhi·ªÅu t·ª´ kh√≥a category
            if noise_count > 0:
                category_in_query = any(keyword in query_lower for keyword in keywords)
                if not category_in_query:
                    penalty += noise_count / len(keywords)
        
        return min(penalty, 1.0)
    
    def _is_truly_relevant(self, query, document):
        """Ki·ªÉm tra document c√≥ th·ª±c s·ª± li√™n quan ƒë·∫øn query kh√¥ng"""
        query_lower = query.lower()
        doc_lower = document.lower()
        
        # N·∫øu query v·ªÅ h·ªçc t·∫≠p m√† document ch·ªâ n√≥i v·ªÅ th·ªÉ thao -> kh√¥ng li√™n quan
        academic_keywords = ['h·ªçc', 'thi', 'ki·ªÉm tra', 'ƒëi·ªÉm', 'm√¥n', 't√≠n ch·ªâ', 'ch∆∞∆°ng tr√¨nh', 'ƒë√†o t·∫°o']
        sports_keywords = ['ƒë√° c·∫ßu', 'b√≥ng b√†n', 'b√≥ng ƒë√°', 'th·ªÉ thao', 'c√¢u l·∫°c b·ªô th·ªÉ thao']
        
        query_is_academic = any(keyword in query_lower for keyword in academic_keywords)
        doc_is_mainly_sports = (
            sum(1 for keyword in sports_keywords if keyword in doc_lower) >= 2 and
            sum(1 for keyword in academic_keywords if keyword in doc_lower) == 0
        )
        
        if query_is_academic and doc_is_mainly_sports:
            return False
        
        # N·∫øu query v·ªÅ quy ƒë·ªãnh m√† document ch·ªâ n√≥i v·ªÅ ho·∫°t ƒë·ªông gi·∫£i tr√≠
        regulation_keywords = ['quy ƒë·ªãnh', 'quy ch·∫ø', 'lu·∫≠t', 'ƒëi·ªÅu', 'kho·∫£n', 'ngh·ªã ƒë·ªãnh']
        entertainment_keywords = ['gi·∫£i tr√≠', 'vui ch∆°i', 'sinh ho·∫°t', 'party', 'l·ªÖ h·ªôi']
        
        query_is_regulation = any(keyword in query_lower for keyword in regulation_keywords)
        doc_is_entertainment = sum(1 for keyword in entertainment_keywords if keyword in doc_lower) >= 2
        
        if query_is_regulation and doc_is_entertainment:
            return False
        
        # Ki·ªÉm tra ƒë·ªô d√†i t·ªëi thi·ªÉu c·ªßa overlap
        query_words = set(re.findall(r'\b\w{3,}\b', query_lower))
        doc_words = set(re.findall(r'\b\w{3,}\b', doc_lower))
        overlap = len(query_words & doc_words)
        
        # C·∫ßn c√≥ √≠t nh·∫•t 1 t·ª´ tr√πng kh·ªõp ho·∫∑c semantic similarity cao
        return overlap >= 1 or len(query_words) == 0
    
    def delete_document(self, doc_id):
        """X√≥a document theo ID (t·ª´ database v√† memory)"""
        try:
            # T√¨m trong database tr∆∞·ªõc
            if doc_id.startswith('db_'):
                db_id = int(doc_id.replace('db_', ''))
                doc = db.session.get(Document, db_id)
                if doc:
                    # X√≥a trong database (soft delete)
                    doc.is_active = False
                    db.session.commit()
                    
                    # X√≥a kh·ªèi memory
                    for i, meta in enumerate(self.metadata):
                        if meta.get('db_id') == db_id:
                            self.documents.pop(i)
                            self.metadata.pop(i)
                            break
                    
                    # Rebuild index
                    self.rebuild_index()
                    return True, f"ƒê√£ x√≥a document {doc_id}"
            
            # Fallback: t√¨m theo ID trong metadata
            for i, meta in enumerate(self.metadata):
                if meta.get('id') == doc_id:
                    # N·∫øu c√≥ db_id th√¨ x√≥a trong database
                    if 'db_id' in meta:
                        doc = db.session.get(Document, meta['db_id'])
                        if doc:
                            doc.is_active = False
                            db.session.commit()
                    
                    # X√≥a kh·ªèi memory
                    self.documents.pop(i)
                    self.metadata.pop(i)
                    
                    # Rebuild index
                    self.rebuild_index()
                    return True, f"ƒê√£ x√≥a document {doc_id}"
            
            return False, "Kh√¥ng t√¨m th·∫•y document"
            
        except Exception as e:
            db.session.rollback()
            return False, f"L·ªói x√≥a document: {str(e)}"
    
    def get_documents_info(self):
        """L·∫•y th√¥ng tin t·∫•t c·∫£ documents"""
        info = []
        for i, (doc, meta) in enumerate(zip(self.documents, self.metadata)):
            info.append({
                'index': i,
                'id': meta.get('id', f'doc_{i}'),
                'preview': doc[:100] + '...' if len(doc) > 100 else doc,
                'length': len(doc),
                'metadata': meta
            })
        return info
    
    def save_index(self):
        """L∆∞u index v√† metadata"""
        try:
            data = {
                'documents': self.documents,
                'metadata': self.metadata,
                'embeddings': self.embeddings,
                'index_data': faiss.serialize_index(self.faiss_index) if self.faiss_index else None,
                'saved_at': datetime.now().isoformat()
            }
            
            with open(self.index_file, 'wb') as f:
                pickle.dump(data, f)
            
            print(f"ƒê√£ l∆∞u index v·ªõi {len(self.documents)} documents")
            return True
        except Exception as e:
            print(f"L·ªói l∆∞u index: {e}")
            return False
    
    def load_index(self):
        """Load index ƒë√£ l∆∞u"""
        try:
            with open(self.index_file, 'rb') as f:
                data = pickle.load(f)
            
            self.documents = data['documents']
            self.metadata = data['metadata']
            
            # X·ª≠ l√Ω embeddings an to√†n
            if 'embeddings' in data and data['embeddings'] is not None:
                embeddings_data = data['embeddings']
                if hasattr(embeddings_data, 'shape'):
                    self.embeddings = embeddings_data
                else:
                    # Convert to numpy array if needed
                    self.embeddings = np.array(embeddings_data)
            else:
                self.embeddings = None
            
            # Load FAISS index
            if 'index_data' in data and data['index_data'] is not None:
                try:
                    self.faiss_index = faiss.deserialize_index(data['index_data'])
                except Exception as faiss_error:
                    print(f"L·ªói load FAISS index: {faiss_error}")
                    self.faiss_index = None
            else:
                self.faiss_index = None
            
            # Ki·ªÉm tra t√≠nh nh·∫•t qu√°n
            if self.embeddings is not None and len(self.documents) != self.embeddings.shape[0]:
                print(f"Warning: Mismatch documents ({len(self.documents)}) vs embeddings ({self.embeddings.shape[0]})")
                self.embeddings = None
                self.faiss_index = None
            
            print(f"ƒê√£ load index v·ªõi {len(self.documents)} documents")
            print(f"Embeddings: {self.embeddings.shape if self.embeddings is not None else 'None'}")
            print(f"FAISS index: {'OK' if self.faiss_index is not None else 'None'}")
            
            return True
        except Exception as e:
            print(f"L·ªói load index: {e}")
            self.embeddings = None
            self.faiss_index = None
            return False
    
    def export_to_file(self, file_path):
        """Export t·∫•t c·∫£ documents ra file"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                for i, doc in enumerate(self.documents):
                    f.write(f"=== Document {i+1} ===\n")
                    f.write(f"ID: {self.metadata[i].get('id', 'N/A')}\n")
                    f.write(f"Added: {self.metadata[i].get('added_date', 'N/A')}\n")
                    f.write(f"Source: {self.metadata[i].get('source', 'N/A')}\n")
                    f.write("Content:\n")
                    f.write(doc)
                    f.write("\n\n")
            return True, f"ƒê√£ export {len(self.documents)} documents"
        except Exception as e:
            return False, f"L·ªói export: {str(e)}"
    
    def debug_status(self):
        """Debug th√¥ng tin hi·ªán t·∫°i c·ªßa DocumentManager"""
        status = {
            'documents_count': len(self.documents),
            'has_embeddings': self.embeddings is not None,
            'embeddings_shape': self.embeddings.shape if self.embeddings is not None else None,
            'has_faiss_index': self.faiss_index is not None,
            'faiss_index_size': self.faiss_index.ntotal if self.faiss_index is not None else None,
            'metadata_count': len(self.metadata),
            'model_available': self.model is not None
        }
        print("DocumentManager Debug Status")
        for key, value in status.items():
            print(f"{key}: {value}")
        return status
