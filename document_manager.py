"""
Document Manager - Qu·∫£n l√Ω ƒë·ªông documents v√† embeddings
"""

import os
import json
import numpy as np
import faiss
import pickle
from datetime import datetime
from models import db, User, Document
import re

class DocumentManager:
    def __init__(self, model, data_dir='data', index_file='search_index.pkl'):
        self.model = model
        self.data_dir = data_dir
        self.index_file = index_file
        self.documents = []
        self.embeddings = None
        self.faiss_index = None
        self.metadata = []
        
        # T·∫°o th∆∞ m·ª•c data n·∫øu ch∆∞a c√≥
        os.makedirs(data_dir, exist_ok=True)
        
        # Load d·ªØ li·ªáu hi·ªán c√≥
        self.load_existing_data()
    
    def load_existing_data(self):
        """Load d·ªØ li·ªáu t·ª´ database"""
        print("ƒêang load d·ªØ li·ªáu t·ª´ database...")
        
        # X√≥a d·ªØ li·ªáu c≈©
        self.documents = []
        self.metadata = []
        self.embeddings = None
        self.faiss_index = None
        
        try:
            # Load t·ª´ database
            db_documents = Document.query.filter_by(is_active=True).order_by(Document.id).all()
            
            if db_documents:
                print(f"üîÑ ƒêang load {len(db_documents)} documents t·ª´ database...")
                for doc in db_documents:
                    self.documents.append(doc.content)
                    self.metadata.append({
                        'id': f'db_{doc.id}',
                        'db_id': doc.id,
                        'source': doc.source_type,
                        'source_file': doc.source_file,
                        'added_date': doc.added_date.isoformat() if doc.added_date else None,
                        'added_by': doc.added_by,
                        'type': 'database',
                        'line_number': doc.line_number,
                        'page_number': doc.page_number
                    })
                print(f"‚úÖ ƒê√£ load {len(self.documents)} documents t·ª´ database")
            else:
                print("üìÑ Database ch∆∞a c√≥ documents n√†o")
                
        except Exception as e:
            print(f"‚ùå L·ªói khi load t·ª´ database: {e}")
        
        # Rebuild index t·ª´ d·ªØ li·ªáu ƒë√£ load
        if self.documents:
            print("üîß Building index t·ª´ documents...")
            self.rebuild_index()
        else:
            print("‚ÑπÔ∏è Kh√¥ng c√≥ documents ƒë·ªÉ build index")
        
        print(f"‚úÖ DocumentManager ƒë√£ s·∫µn s√†ng v·ªõi {len(self.documents)} documents")
    

    


    
    def add_text_document(self, text, metadata=None, user_id=None):
        """Th√™m document text m·ªõi (l∆∞u v√†o database)"""
        if not text or not text.strip():
            return False, "N·ªôi dung r·ªóng"
        
        try:
            # T·∫°o source_info t·ª´ metadata
            source_info = metadata if metadata else {}
            
            # T·∫°o document trong database
            doc = Document.create_from_text(
                content=text.strip(),
                source_type='manual',
                source_file=None,
                source_info=source_info,
                added_by=user_id
            )
            
            # L∆∞u v√†o database
            db.session.add(doc)
            db.session.commit()
            
            # Th√™m v√†o memory ƒë·ªÉ c√≥ th·ªÉ search ngay
            self.documents.append(text.strip())
            doc_metadata = {
                'id': f'db_{doc.id}',
                'db_id': doc.id,
                'source': 'manual',
                'added_date': doc.added_date.isoformat(),
                'added_by': user_id,
                'type': 'database',
                'length': len(text.strip())
            }
            self.metadata.append(doc_metadata)
            
            # C·∫≠p nh·∫≠t index
            success = self._add_to_index(text.strip())
            
            if success:
                return True, f"ƒê√£ th√™m document ID: {doc.id}"
            else:
                # Rollback n·∫øu l·ªói index
                self.documents.pop()
                self.metadata.pop()
                return False, "L·ªói khi c·∫≠p nh·∫≠t search index"
                
        except Exception as e:
            db.session.rollback()
            return False, f"L·ªói l∆∞u database: {str(e)}"
    
    def add_documents_from_pdf_file(self, file_path, user_id=None):
        """Th√™m documents t·ª´ file PDF (l∆∞u v√†o database)"""
        if not os.path.exists(file_path):
            return False, "File kh√¥ng t·ªìn t·∫°i"
        
        try:
            if file_path.endswith('.pdf'):
                documents = self._extract_from_pdf(file_path)
            else:
                return False, "Ch·ªâ h·ªó tr·ª£ file .pdf"
            
            if not documents:
                return False, "Kh√¥ng t√¨m th·∫•y n·ªôi dung trong file PDF"
            
            # Th√™m t·ª´ng document v√†o database
            added_count = 0
            source_file = os.path.basename(file_path)
            
            for i, doc_text in enumerate(documents):
                if len(doc_text.strip()) > 50:  # Ch·ªâ th√™m document ƒë·ªß d√†i
                    try:
                        # T·∫°o document trong database
                        doc = Document.create_from_text(
                            content=doc_text.strip(),
                            source_type='pdf',
                            source_file=source_file,
                            source_info={'pdf_extract_index': i},
                            added_by=user_id,
                            page_number=i + 1  # Gi·∫£ ƒë·ªãnh m·ªói document t·ª´ 1 page
                        )
                        
                        db.session.add(doc)
                        db.session.commit()
                        
                        # Th√™m v√†o memory
                        self.documents.append(doc_text.strip())
                        self.metadata.append({
                            'id': f'db_{doc.id}',
                            'db_id': doc.id,
                            'source': 'pdf',
                            'source_file': source_file,
                            'added_date': doc.added_date.isoformat(),
                            'added_by': user_id,
                            'type': 'database',
                            'page_number': i + 1
                        })
                        
                        # C·∫≠p nh·∫≠t index
                        self._add_to_index(doc_text.strip())
                        added_count += 1
                        
                    except Exception as doc_error:
                        print(f"L·ªói th√™m document {i}: {doc_error}")
                        db.session.rollback()
                        continue
            
            return True, f"ƒê√£ th√™m {added_count}/{len(documents)} documents t·ª´ file PDF"
            
        except Exception as e:
            return False, f"L·ªói x·ª≠ l√Ω file PDF: {str(e)}"
    

    
    def _extract_from_txt(self, file_path):
        """Tr√≠ch xu·∫•t t·ª´ file txt"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Chia th√†nh c√°c paragraph
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        # N·∫øu kh√¥ng c√≥ paragraph, chia theo d√≤ng
        if len(paragraphs) <= 1:
            paragraphs = [line.strip() for line in content.split('\n') if line.strip()]
        
        return paragraphs
    
    def _extract_from_pdf(self, file_path):
        """Tr√≠ch xu·∫•t t·ª´ file PDF (s·ª≠ d·ª•ng PyPDF2 nh∆∞ trong app.py)"""
        try:
            import PyPDF2
            
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text_content = []
                
                for page in pdf_reader.pages:
                    page_text = page.extract_text()
                    if page_text and page_text.strip():
                        # Chia th√†nh c√°c paragraph
                        paragraphs = page_text.split('\n\n')
                        for para in paragraphs:
                            para = para.strip()
                            if len(para) > 50:  # Ch·ªâ l·∫•y paragraph ƒë·ªß d√†i
                                text_content.append(para)
                
                return text_content
        except Exception as e:
            print(f"L·ªói ƒë·ªçc file PDF: {e}")
            return []


    
    def _add_to_index(self, text):
        """Th√™m m·ªôt document v√†o index hi·ªán c√≥"""
        try:
            # T·∫°o embedding cho text m·ªõi
            new_embedding = self.model.encode([text], convert_to_tensor=True)
            new_embedding_np = new_embedding.cpu().numpy()
            
            if self.faiss_index is None:
                # T·∫°o index m·ªõi n·∫øu ch∆∞a c√≥
                embedding_dim = new_embedding_np.shape[1]
                self.faiss_index = faiss.IndexFlatL2(embedding_dim)
                self.embeddings = new_embedding_np
            else:
                # Th√™m v√†o index hi·ªán c√≥
                self.embeddings = np.vstack([self.embeddings, new_embedding_np])
            
            # Th√™m v√†o FAISS index
            self.faiss_index.add(new_embedding_np)
            
            return True
        except Exception as e:
            print(f"L·ªói th√™m v√†o index: {e}")
            return False
    
    def rebuild_index(self):
        """Rebuild to√†n b·ªô index t·ª´ documents hi·ªán c√≥"""
        if not self.documents:
            print("‚ùå Kh√¥ng c√≥ documents ƒë·ªÉ rebuild")
            return
        
        print(f"üîß Rebuilding index cho {len(self.documents)} documents...")
        
        try:
            # Reset tr∆∞·ªõc khi rebuild
            self.embeddings = None
            self.faiss_index = None
            
            # T·∫°o embeddings cho t·∫•t c·∫£ documents
            print("üîÑ T·∫°o embeddings...")
            embeddings = self.model.encode(self.documents, convert_to_tensor=True)
            self.embeddings = embeddings.cpu().numpy()
            print(f"‚úÖ Embeddings shape: {self.embeddings.shape}")
            
            # T·∫°o FAISS index m·ªõi
            print("üîÑ T·∫°o FAISS index...")
            embedding_dim = self.embeddings.shape[1]
            self.faiss_index = faiss.IndexFlatL2(embedding_dim)
            self.faiss_index.add(self.embeddings)
            print(f"‚úÖ FAISS index v·ªõi {self.faiss_index.ntotal} vectors")
            
            print(f"‚úÖ Rebuild ho√†n th√†nh: {len(self.documents)} documents")
            
            # L∆∞u index
            self.save_index()
            
        except Exception as e:
            print(f"‚ùå L·ªói rebuild index: {e}")
            # Reset v·ªÅ tr·∫°ng th√°i an to√†n
            self.embeddings = None
            self.faiss_index = None
    

    
    def search(self, query, k=5, similarity_threshold=0.3):
        """T√¨m ki·∫øm documents"""
        if self.faiss_index is None or len(self.documents) == 0:
            return []
        
        # Ki·ªÉm tra ƒë·ªìng b·ªô gi·ªØa documents v√† embeddings
        if hasattr(self, 'embeddings') and self.embeddings is not None and len(self.documents) != self.embeddings.shape[0]:
            print(f"Warning: Documents ({len(self.documents)}) v√† embeddings ({self.embeddings.shape[0]}) kh√¥ng ƒë·ªìng b·ªô. Rebuilding index...")
            self.rebuild_index()
        
        try:
            # T·∫°o embedding cho query
            query_embedding = self.model.encode([query], convert_to_tensor=True)
            query_np = query_embedding.cpu().numpy()
            
            # T√¨m ki·∫øm v·ªõi FAISS
            distances, indices = self.faiss_index.search(query_np, min(k, len(self.documents)))
        except Exception as e:
            print(f"‚ùå L·ªói trong qu√° tr√¨nh search: {e}")
            return []
        
        results = []
        for i, idx in enumerate(indices[0]):
            if idx >= len(self.documents):
                continue
            
            # T√≠nh cosine similarity
            doc_embedding = self.embeddings[idx:idx+1]
            
            # Ki·ªÉm tra embedding h·ª£p l·ªá
            if len(doc_embedding) == 0 or len(doc_embedding[0]) == 0:
                continue
                
            doc_norm = np.linalg.norm(doc_embedding[0])
            query_norm = np.linalg.norm(query_np[0])
            
            if doc_norm == 0 or query_norm == 0:
                continue
                
            similarity = np.dot(query_np[0], doc_embedding[0]) / (query_norm * doc_norm)
            
            if similarity >= similarity_threshold:
                results.append({
                    'index': int(idx),
                    'content': self.documents[idx],
                    'metadata': self.metadata[idx] if idx < len(self.metadata) else {},
                    'score': float(distances[0][i]),
                    'similarity': float(similarity),
                    'rank': len(results) + 1,
                    'source': 'default'
                })
        
        return results
    
    def delete_document(self, doc_id):
        """X√≥a document theo ID (t·ª´ database v√† memory)"""
        try:
            # T√¨m trong database tr∆∞·ªõc
            if doc_id.startswith('db_'):
                db_id = int(doc_id.replace('db_', ''))
                doc = db.session.get(Document, db_id)
                if doc:
                    # X√≥a trong database (soft delete)
                    doc.is_active = False
                    db.session.commit()
                    
                    # X√≥a kh·ªèi memory
                    for i, meta in enumerate(self.metadata):
                        if meta.get('db_id') == db_id:
                            self.documents.pop(i)
                            self.metadata.pop(i)
                            break
                    
                    # Rebuild index
                    self.rebuild_index()
                    return True, f"ƒê√£ x√≥a document {doc_id}"
            
            # Fallback: t√¨m theo ID trong metadata
            for i, meta in enumerate(self.metadata):
                if meta.get('id') == doc_id:
                    # N·∫øu c√≥ db_id th√¨ x√≥a trong database
                    if 'db_id' in meta:
                        doc = db.session.get(Document, meta['db_id'])
                        if doc:
                            doc.is_active = False
                            db.session.commit()
                    
                    # X√≥a kh·ªèi memory
                    self.documents.pop(i)
                    self.metadata.pop(i)
                    
                    # Rebuild index
                    self.rebuild_index()
                    return True, f"ƒê√£ x√≥a document {doc_id}"
            
            return False, "Kh√¥ng t√¨m th·∫•y document"
            
        except Exception as e:
            db.session.rollback()
            return False, f"L·ªói x√≥a document: {str(e)}"
    
    def get_documents_info(self):
        """L·∫•y th√¥ng tin t·∫•t c·∫£ documents"""
        info = []
        for i, (doc, meta) in enumerate(zip(self.documents, self.metadata)):
            info.append({
                'index': i,
                'id': meta.get('id', f'doc_{i}'),
                'preview': doc[:100] + '...' if len(doc) > 100 else doc,
                'length': len(doc),
                'metadata': meta
            })
        return info
    
    def save_index(self):
        """L∆∞u index v√† metadata"""
        try:
            data = {
                'documents': self.documents,
                'metadata': self.metadata,
                'embeddings': self.embeddings,
                'index_data': faiss.serialize_index(self.faiss_index) if self.faiss_index else None,
                'saved_at': datetime.now().isoformat()
            }
            
            with open(self.index_file, 'wb') as f:
                pickle.dump(data, f)
            
            print(f"ƒê√£ l∆∞u index v·ªõi {len(self.documents)} documents")
            return True
        except Exception as e:
            print(f"L·ªói l∆∞u index: {e}")
            return False
    
    def load_index(self):
        """Load index ƒë√£ l∆∞u"""
        try:
            with open(self.index_file, 'rb') as f:
                data = pickle.load(f)
            
            self.documents = data['documents']
            self.metadata = data['metadata']
            
            # X·ª≠ l√Ω embeddings an to√†n
            if 'embeddings' in data and data['embeddings'] is not None:
                embeddings_data = data['embeddings']
                if hasattr(embeddings_data, 'shape'):
                    self.embeddings = embeddings_data
                else:
                    # Convert to numpy array if needed
                    self.embeddings = np.array(embeddings_data)
            else:
                self.embeddings = None
            
            # Load FAISS index
            if 'index_data' in data and data['index_data'] is not None:
                try:
                    self.faiss_index = faiss.deserialize_index(data['index_data'])
                except Exception as faiss_error:
                    print(f"L·ªói load FAISS index: {faiss_error}")
                    self.faiss_index = None
            else:
                self.faiss_index = None
            
            # Ki·ªÉm tra t√≠nh nh·∫•t qu√°n
            if self.embeddings is not None and len(self.documents) != self.embeddings.shape[0]:
                print(f"Warning: Mismatch documents ({len(self.documents)}) vs embeddings ({self.embeddings.shape[0]})")
                self.embeddings = None
                self.faiss_index = None
            
            print(f"ƒê√£ load index v·ªõi {len(self.documents)} documents")
            print(f"Embeddings: {self.embeddings.shape if self.embeddings is not None else 'None'}")
            print(f"FAISS index: {'OK' if self.faiss_index is not None else 'None'}")
            
            return True
        except Exception as e:
            print(f"L·ªói load index: {e}")
            self.embeddings = None
            self.faiss_index = None
            return False
    
    def export_to_file(self, file_path):
        """Export t·∫•t c·∫£ documents ra file"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                for i, doc in enumerate(self.documents):
                    f.write(f"=== Document {i+1} ===\n")
                    f.write(f"ID: {self.metadata[i].get('id', 'N/A')}\n")
                    f.write(f"Added: {self.metadata[i].get('added_date', 'N/A')}\n")
                    f.write(f"Source: {self.metadata[i].get('source', 'N/A')}\n")
                    f.write("Content:\n")
                    f.write(doc)
                    f.write("\n\n")
            return True, f"ƒê√£ export {len(self.documents)} documents"
        except Exception as e:
            return False, f"L·ªói export: {str(e)}"
    
    def debug_status(self):
        """Debug th√¥ng tin hi·ªán t·∫°i c·ªßa DocumentManager"""
        status = {
            'documents_count': len(self.documents),
            'has_embeddings': self.embeddings is not None,
            'embeddings_shape': self.embeddings.shape if self.embeddings is not None else None,
            'has_faiss_index': self.faiss_index is not None,
            'faiss_index_size': self.faiss_index.ntotal if self.faiss_index is not None else None,
            'metadata_count': len(self.metadata),
            'model_available': self.model is not None
        }
        print("=== DocumentManager Debug Status ===")
        for key, value in status.items():
            print(f"{key}: {value}")
        print("=====================================")
        return status
